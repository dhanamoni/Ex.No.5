# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: 
To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.


### OUTPUT

## Step 1: Define Prompting Patterns

1. **Broad / Unstructured Prompt**

   * Example: *“Tell me about climate change.”*
   * Leaves interpretation open, testing creativity and generalization.

2. **Basic / Clear Prompt**

   * Example: *“Explain climate change in simple terms suitable for a 10-year-old.”*
   * Adds clarity, guiding the model toward a target audience or output style.

3. **Refined / Specific Prompt**

   * Example: *“Explain the main causes of climate change in 3 bullet points, and suggest 2 solutions.”*
   * Highly structured, testing the model’s ability to follow detailed instructions.

---

## Step 2: Choose Scenarios for Testing

We’ll need diverse scenarios to reveal differences in model performance:

1. **Knowledge recall** – Factual explanation (e.g., history, science, tech).
2. **Creative generation** – Storytelling, brainstorming.
3. **Analytical reasoning** – Compare/contrast, problem-solving.
4. **Instruction following** – Step-by-step tasks, formatting.

---

## Step 3: Evaluation Criteria

We’ll analyze responses based on:

| **Criteria**   | **What We Look For**                              |
| -------------- | ------------------------------------------------- |
| **Quality**    | Grammar, fluency, readability                     |
| **Accuracy**   | Correctness of facts, logical soundness           |
| **Depth**      | Level of detail, nuance, or insight               |
| **Relevance**  | Staying on-topic with minimal filler              |
| **Structure**  | Whether the response follows instructions         |
| **Creativity** | Originality and engaging style (where applicable) |

---

## Step 4: Comparative Analysis Example

**Scenario: Explain Blockchain Technology**

| Prompt Style | Example Prompt                                                                       | Expected Response Traits                                  | Model Tendencies                                                            |
| ------------ | ------------------------------------------------------------------------------------ | --------------------------------------------------------- | --------------------------------------------------------------------------- |
| **Broad**    | “What is blockchain?”                                                                | General, varied depth depending on model; may lack focus. | Risk of oversimplification or info-dump.                                    |
| **Basic**    | “Explain blockchain in simple terms with an example.”                                | Clear, audience-specific, usually more accurate.          | Balanced mix of depth and simplicity.                                       |
| **Refined**  | “List 3 key features of blockchain, and explain its use in supply chain management.” | Highly structured, targeted info.                         | Strong compliance if model follows instructions; depth depends on training. |

---

## Step 5: Insights

* **Broad prompts** → Test model’s *default reasoning & creativity*, but responses may be vague.
* **Basic prompts** → Improve *accuracy and clarity* by narrowing the scope.
* **Refined prompts** → Produce *structured, reliable outputs* but may limit creativity.

# RESULT: 
The project was completed successfully within the deadline
